| PLMs       | Hidden size | Hidden layers | Attention heads | Parameters | Download Link                                                                 |
|------------|-------------|---------------|-----------------|------------|--------------------------------------------------------------------------------|
| BERT       | 768         | 12            | 12              | 110M       | [BERT Base](https://huggingface.co/bert-base-uncased)                          |
| BERT-WWM   | 768         | 12            | 12              | 110M       | [BERT-WWM Base](https://github.com/ymcui/Chinese-BERT-wwm)                     |
| RoBERTa    | 768         | 12            | 12              | 125M       | [RoBERTa Base](https://huggingface.co/roberta-base)                            |
| ERNIE      | 768         | 12            | 12              | 110M       | [ERNIE Base](https://github.com/PaddlePaddle/ERNIE)                            |
| LERT       | 768         | 12            | 12              | 110M       | 具体下载链接可能需查阅相关文献或实现文档，但通常与BERT Base类似。 |
